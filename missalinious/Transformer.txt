Below is a complete, modular SystemVerilog implementation of a small but practical Transformer encoder block (Multi-Head Self-Attention + Feed-Forward + LayerNorm + residuals), plus a functional testbench.
The RTL is written to be clear and modular; many modules are synthesizable (matrix multiply, linear projection, layernorm, FFN). The softmax used inside attention is implemented behaviorally (using real/$exp) for correctness in simulation — it is NOT synthesizable. I’ve marked that clearly and included notes and a simple replacement strategy for synthesis (LUT / piecewise approximation / fixed-point implementation).

If you want a fully synthesizable softmax implementation (fixed-point, LUT or CORDIC/linear-approx), tell me and I’ll provide one next.

Features

Parameterizable EMB (embedding dim), HEADS, SEQ (sequence length), FF_DIM, DATA_W (bit width).

Multi-Head Self-Attention module (Q/K/V projections, dot-product attention, output projection).

Scaled dot-product attention with behavioral softmax for simulation correctness.

Feed-Forward Network (two linear layers + GELU (approx) or ReLU).

LayerNorm implementation (synthesizable).

Residual connections and dropout stubs (no randomness).

Testbench that runs a tiny example (SEQ=4, EMB=8, HEADS=2) and checks shapes/flows.

Clear notes on what is synthesizable and how to replace simulation-only parts.

Important: The softmax_unit in this package is behavioral and uses real/$exp for simulation accuracy. Replace it with a fixed-point LUT or an approximate softmax implementation for synthesis. I included a softmax_stub placeholder and guidance comments.

Files included below (single reply)

transformer_pkg.sv — parameter package & typedefs

linear.sv — parameterized dense (matrix × vector) module (synthesizable)

layernorm.sv — Layer Normalization (synthesizable)

softmax_behav.sv — behavioral softmax for simulation (NOT synthesizable)

mha.sv — Multi-Head Attention (connects linear, softmax)

ffn.sv — Feed-Forward Network (2-layer dense + activation)

transformer_block.sv — Assembles one Transformer encoder block

tb_transformer.sv — Testbench (stimulus + checks)
